{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lalitpandey02/PythonNotebooks/blob/main/1_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olsrp2BvDw_T"
      },
      "source": [
        "<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gldqBw-Zw5z"
      },
      "source": [
        "**<center><h1>Object Detection</center>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG0HNuQTTL7d"
      },
      "source": [
        "---\n",
        "# **Table of Contents**\n",
        "---\n",
        "\n",
        "**1.** [**Categorization of Object Detection Tasks**](#section1)<br>\n",
        "  - **1.1** [**Image Classification**](#section101)\n",
        "  - **1.2** [**Object Classification and Localization**](#section102)\n",
        "  - **1.3** [**Multiple Objects Detection and Localization**](#section103)\n",
        "  \n",
        "**2.** [**YOLO: You Only Look Once**](#section2)<br>\n",
        "**3.** [**SSD: Single Shot MultiBox Detection**](#section3)<br>\n",
        "**4.** [**Conclusion**](#section4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkP6ofJrTL7e"
      },
      "source": [
        "---\n",
        "<a name = Section1></a>\n",
        "# **1. Categorization of Object Detection Tasks**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEKZo_tWVHR7"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/cv_tasks.png\" width=\"900\" height=\"450\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhMkdE-ZTL7f"
      },
      "source": [
        "<a id=section101></a>\n",
        "### **1.1 Image Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMmuf6ttVHR9"
      },
      "source": [
        "- This is the most **common** computer vision problem where an algorithm looks at an image and **classifies** the **object** in it.\n",
        "\n",
        "- Image classification has a **wide variety** of applications, ranging from **face detection** on social networks to **cancer detection** in medicine.\n",
        "\n",
        "- Such problems are typically modeled using **Convolutional Neural Nets** (CNNs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH-r6y3FVHR-"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/img_classification.png\" width=\"900\" height=\"500\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZEzdFU1VHR-"
      },
      "source": [
        "<a id=section102></a>\n",
        "### **1.2 Object Classification and Localization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAyd6RRiVHR_"
      },
      "source": [
        "- Let’s say we not only want to know whether there is **cat** in the image, but **where** exactly is the cat.\n",
        "\n",
        "- Object **localization** algorithms not only **label** the **class** of an object, but also **draw** a **bounding box** around **position** of object in the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lRR9oRmVHSA"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/object_localization.png\" width=\"900\" height=\"500\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38JKRZ8xVHSA"
      },
      "source": [
        "- Now, to make our model **draw** the **bounding boxes** of an object, we just change the output labels from the previous algorithm, so as to make our **model learn** the **class** of object and also the **position** of the object in the image. \n",
        "\n",
        "- We **add 4** more **numbers** in the **output layer** which include **centroid position** of the object and proportion of **width** and **height** of **bounding box** in the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hx6xpRHVHSB"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/simple_right.jpg\" width=\"400\" height=\"300\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxXYqNtLVHSC"
      },
      "source": [
        "- Just **add** a bunch of **output units** to spit out the **x, y coordinates** of different positions you want to **recognize**.\n",
        "\n",
        "- These different **positions** or landmark would be **consistent** for a particular object in all the images we have.\n",
        "\n",
        "- For e.g. for a car, **height** would be **smaller than width** and **centroid** would have some specific **pixel density** as compared to other points in the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udIX9v9lVHSC"
      },
      "source": [
        "<a id=section103></a>\n",
        "### **1.3 Multiple Objects Detection and Localization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xM3FLCkVHSD"
      },
      "source": [
        "- What if there are **multiple objects** in the image (3 dogs and 2 cats as in above figure) and we want to **detect** them **all**?\n",
        "\n",
        "- That would be an **Object Detection and Localization** problem. \n",
        "\n",
        "- A well known application of this is in **self-driving cars** where the algorithm not only needs to detect the cars, but also pedestrians, motorcycles, trees and other **objects** in the frame.\n",
        "\n",
        "- These kind of problems need to **leverage** the ideas or concepts learnt from **image classification** as well as from **object localization**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oooQX8TEVHSE"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/multiple_object_localization.png\" width=\"900\" height=\"500\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt6HxEu6VHSE"
      },
      "source": [
        "- To **detect all** kinds of **objects** in an image, we can directly use what we learnt so far from object localization.\n",
        "\n",
        "- The difference is that we want our algorithm to be able to **classify** and **localize** all the **objects** in an image, not just one.\n",
        "\n",
        "- So the idea is, just **crop** the **image** into multiple images and **run CNN** for all the cropped images to **detect** an object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFQINmnQVHSF"
      },
      "source": [
        "- This solution is known as **Object Detection** with Sliding Windows. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyxWY2QzVHSG"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/sliding_window.gif\" width=\"500\" height=\"350\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLyLeZ53VHSG"
      },
      "source": [
        "- It is very basic solution which has many caveats as the following:\n",
        "\n",
        "\n",
        "- **Computationally expensive**: Cropping multiple images and passing it through ConvNet is going to be computationally very expensive.\n",
        "\n",
        "  - **Solution**: There is a simple hack to **improve** the computation power of sliding window method. \n",
        "  \n",
        "  - It is to **replace** the **fully connected layer** in ConvNet with **1x1 convolution layers** and for a given window size, pass the input image only once.\n",
        "  \n",
        "  - So, in actual implementation we do not pass the cropped images one at a time, but we **pass** the **complete image** at once.\n",
        "\n",
        "\n",
        "- **Inaccurate bounding boxes**: We are sliding windows of square shape all over the image, maybe the object is **rectangular** or maybe none of the **squares** match perfectly with the **actual size** of the object. \n",
        "\n",
        "  - Although this algorithm has ability to find and **localize** multiple objects in an image, but the **accuracy** of **bounding box** is still **bad**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XVAK3WdVHSH"
      },
      "source": [
        "- So, how can we make our **algorithm better** and **faster**?\n",
        "\n",
        "- It turns out that we have **YOLO (You Only Look Once)** which is much more **accurate** and **faster** than the sliding window algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1XOdiEVHSH"
      },
      "source": [
        "---\n",
        "<a name = Section2></a>\n",
        "# **2. YOLO: You Only Look Once**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYi5jn-HVHSI"
      },
      "source": [
        "- It is based on only a **minor tweak** on the top of algorithms that we already know.\n",
        "\n",
        "- The idea is to **divide** the **image into multiple grids**. \n",
        "\n",
        "- Then we **change** the **label** of our data such that we implement both **localization** and **classificatio**n algorithm for **each grid cell**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdZFefI4VHSJ"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/yolo.png\" width=\"900\" height=\"500\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFZrxk1U-dRu"
      },
      "source": [
        "#### **YOLO, in easy steps:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AGYxEaqVHSJ"
      },
      "source": [
        "1. **Divide the image into multiple grids**. \n",
        "\n",
        "  - For illustration, we have drawn 4x4 grids in above figure, but actual implementation of **YOLO** has different number of grids. (**7x7** for **training** YOLO on PASCAL VOC dataset)\n",
        "\n",
        "2. **Label the training data** as shown in the above figure. \n",
        "\n",
        "  - If $C$ is number of **unique** objects in our data, $S*S$ is number of grids into which we split our image, then our **output vector** will be of **length** $S*S*(C+5)$. \n",
        "  \n",
        "  - For e.g. in above case, our target vector is $4*4*(3+5)$ as we divided our images into 4x4 grids and are **training** for 3 unique objects: Car, Light and Pedestrian.\n",
        "\n",
        "3. Make **one deep convolutional** neural net with **loss function** as **error between output activations** and label vector. \n",
        "\n",
        "  - Basically, the model predicts the **output** of all the grids in just **one forward pass** of input image through **ConvNet**.\n",
        "\n",
        "4. Keep in mind that the **label** for object being present in a grid cell (**`P.Object`**) is determined by the presence of **object’s centroid** in that grid. \n",
        "\n",
        "  - This is **important** to not allow one object to be **counted multiple times** in different grids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ry4b8xG-iU5"
      },
      "source": [
        "#### **Caveats of YOLO and their solutions:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEm_NOdjVHSK"
      },
      "source": [
        "1. **Can’t detect multiple objects in same grid**.\n",
        "\n",
        "  - This issue can be solved by choosing **smaller grid size**. \n",
        "  \n",
        "  - But even by choosing **smaller** grid size, the algorithm can still fail in cases where **objects** are very close to each other, like image of **flock** of birds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAp0Ig0nVHSL"
      },
      "source": [
        "- **Solution**: **Anchor boxes**. \n",
        "  \n",
        "  - In addition to having $5+C$ labels for each grid cell (where $C$ is number of distinct objects), the idea of anchor boxes is to have $(5+C)*A$ labels for each grid cell, where $A$ is **required** anchor boxes. \n",
        "  \n",
        "  - If one **object** is assigned to one anchor box in **one grid**, other object can be assigned to the other **anchor box** of same grid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAFpjqUcVHSL"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/yolo_anchor_boxes.png\" width=\"850\" height=\"550\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSUqgSoYVHSM"
      },
      "source": [
        "**2.** **Possibility to detect one object multiple times**.\n",
        "\n",
        "- **Solution**: **Non-max suppression**. \n",
        "  \n",
        "  - Non max suppression **removes** the **low probability bounding boxes** which are very close to a high probability bounding boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSeuJdgsVHSM"
      },
      "source": [
        "---\n",
        "<a name = Section3></a>\n",
        "# **3. SSD: Single Shot MultiBox Detection**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zf9WJnsVHSN"
      },
      "source": [
        "- **Reasons behind the origin of SSD algorithm:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjOlzUgDVHSO"
      },
      "source": [
        "1. **How do you know the size of the sliding window so that it always contains the object?** \n",
        "\n",
        "  - Different types of **objects** (palm tree and swimming pool), even the same type of objects (e.g. a small building and a large buidling) can be of **varying** sizes as well.\n",
        "\n",
        "2. **Aspect ratio** (the ratio of height to width of a bounding box). \n",
        "\n",
        "  - A lot of objects can be present in **various shapes** like a building footprint will have a different **aspect ratio** than a palm tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UmpaC6cVHSO"
      },
      "source": [
        "- To solve these problems, we would have to try out **different sizes/shapes** of **sliding window**, which is very **computationally intensive**, especially with deep neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4V2TKHRVHSP"
      },
      "source": [
        "#### **Single-Shot Detector (SSD)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NYIM0nPVHSP"
      },
      "source": [
        "- SSD has two components: a **backbone model** and **SSD head**.\n",
        "\n",
        "- **Backbone** model usually is a **pre-trained image classification network** as a feature extractor. \n",
        "\n",
        "- This is typically a network like **ResNet** trained on ImageNet from which the final fully connected classification layer has been removed. \n",
        "\n",
        "- We are thus left with a deep neural network that is able to **extract semantic meaning** from the input image while **preserving** the **spatial structure** of the image albeit at a lower resolution.\n",
        "\n",
        "- For ResNet34, the backbone results in a **256 7x7 feature maps** for an input image. \n",
        "\n",
        "- The **SSD head** is just one or more **convolutional layers** added to this backbone and the outputs are interpreted as the **bounding boxes** and classes of objects in the spatial location of the final layers activations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgrbowGxVHSQ"
      },
      "source": [
        "- In the figure below, the first few layers (**white boxes**) are the **backbone**, the last few layers (**blue boxes**) represent the **SSD head**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3ZgnWNSVHSR"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/ssd.png\" width=\"850\" height=\"250\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h1Z-k26VHSS"
      },
      "source": [
        "#### **Grid Cell**\n",
        "\n",
        "- Instead of using **sliding window**, SSD divides the image using a **grid** and have each grid cell be responsible for detecting objects in that **region** of the image.\n",
        "\n",
        "- **Detecting** objects simply means **predicting** the **class** and **location** of an object within that region.\n",
        "\n",
        "- If **no object** is present, we consider it as the **background** class and the **location** is **ignored**.\n",
        "\n",
        "- For instance, we could use a **4x4 grid** in the example below. \n",
        "\n",
        "- Each grid cell is able to output the **position** and shape of the object it contains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odCzPWGOVHSS"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/grid_cell.png\" width=\"400\" height=\"350\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLNHvrWiVHST"
      },
      "source": [
        "- Now what if there are **multiple objects** in one grid cell or we need to detect multiple objects of different shapes. \n",
        "\n",
        "- There is where anchor **box** and **receptive** field come into play."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Psc7Z35VHSU"
      },
      "source": [
        "#### **Anchor Box**\n",
        "\n",
        "- Each grid cell in SSD can be assigned with **multiple anchor/prior** boxes.\n",
        "\n",
        "- These anchor boxes are **pre-defined** and each one is responsible for a **size** and **shape** within a grid cell.\n",
        "\n",
        "- For example, the swimming pool in the image below corresponds to the taller **anchor box** while the building corresponds to the **wider box**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZ0LOeSVHSU"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/anchor_box.png\" width=\"600\" height=\"350\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_esiwywVHSV"
      },
      "source": [
        "- SSD uses a matching phase while training, to **match** the **appropriate anchor box** with the bounding boxes of each ground truth object within an image.\n",
        "\n",
        "- Essentially, the anchor box with the **highest degree** of **overlap** with an object is responsible for **predicting** that object’s class and its location.\n",
        "\n",
        "- This property is used for **training** the network and for **predicting** the detected objects and their **locations** once the network has been trained.\n",
        "\n",
        "- In practice, each **anchor box** is specified by an **aspect ratio** and a **zoom level**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R0s2mTWVHSW"
      },
      "source": [
        "#### **Aspect Ratio**\n",
        "\n",
        "- Not all objects are **square** in shape.\n",
        "\n",
        "- Some are **longer** and some are **wider**, by varying degrees.\n",
        "\n",
        "- The SSD architecture allows **pre-defined aspect ratios** of the anchor boxes to account for this.\n",
        "\n",
        "- The **ratios parameter** can be used to specify the **different aspect ratios** of the anchor boxes **associates** with each grid cell at each **zoom/scale level**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGew7Ds6VHSW"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/aspect_ratio.png\" width=\"400\" height=\"350\"/></center>\n",
        "<br> \n",
        "<center><strong>The bounding box of building 1 is higher, while the bouding box for building 2 is wider</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdrZwRmaVHSX"
      },
      "source": [
        "#### **Zoom Level**\n",
        "\n",
        "- It is not necessary for the **anchor boxes** to have the **same size** as the grid cell.\n",
        "\n",
        "- We might be interested in finding **smaller** or **larger** objects within a grid cell.\n",
        "\n",
        "- The zooms parameter is used to **specify** how much the anchor boxes need to be **scaled up** or **down** with respect to each grid cell.\n",
        "\n",
        "- Just like what we have seen in the anchor box example, the size of **building** is generally larger than swimming pool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhVHDvyeVHSX"
      },
      "source": [
        "#### **Receptive Field**\n",
        "\n",
        "- Receptive field is defined as the **region** in the input space that a particular **CNN’s** feature is looking at (i.e. be affected by).\n",
        "\n",
        "- Because of the convolution operation, **features** at different layers represent **different sizes** of region in the **input image**.\n",
        "\n",
        "- As it goes deeper, the **size** represented by a feature gets **larger**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m8gwKEIVHSY"
      },
      "source": [
        "- In this example below, we start with the **bottom layer** (5x5) and then apply a convolution that results in the **middle layer** (3x3) where one feature (green pixel) represents a 3x3 region of the **input layer** (bottom layer).\n",
        "\n",
        "- And then apply the **convolution** to middle layer and get the top layer (2x2) where each feature corresponds to a **7x7 region** on the input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJIAlqa6VHSZ"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/receptive_field.png\" width=\"760\" height=\"450\"/></center>\n",
        "<br> \n",
        "<center><strong>Visualizing CNN feature maps and receptive field</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4DKetV1VHSY"
      },
      "source": [
        "- These kind of green and orange 2D array are also called **feature maps** which refer to a set of features created by applying the same **feature extractor** at **different locations** of the input map in a sliding window fashion.\n",
        "\n",
        "- Features in the same feature map have the **same receptive field** and look for the **same pattern** but at **different locations**.\n",
        "\n",
        "- This creates the **spatial invariance** of ConvNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i2xj9d6VHSa"
      },
      "source": [
        "- Receptive field is the central premise of the **SSD architecture** as it enables us to detect objects at **different scales** and output a **tighter bounding box**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_rlHrR-W68f"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/why.jpg\" width=\"500\" height=\"300\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqH8iv6fW7-1"
      },
      "source": [
        "- As you might still remember, the **ResNet34** backbone outputs a 256 **7x7** feature maps for an **input** image.\n",
        "\n",
        "- If we specify a **4x4** grid, the simplest approach is just to **apply** a convolution to this **feature map** and convert it to 4x4.\n",
        "\n",
        "- This approach can actually work to some **extent** and is exatcly the idea of **YOLO** (You Only Look Once)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcdp2au2VHSa"
      },
      "source": [
        "- The extra step taken by SSD is that it applies more **convolutional** layers to the backbone **feature map** and has each of these convolution layers **output** a object detection **results**.\n",
        "\n",
        "- As earlier layers bearing **smaller receptive field** can represent smaller sized objects, predictions from earlier layers **help** in dealing with **smaller sized objects**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMBJkwRrVHSb"
      },
      "source": [
        "- Because of this, SSD allows us to define a **hierarchy** of **grid cells** at different layers.\n",
        "\n",
        "- For example, we could use a **4x4 grid** to find smaller objects, a **2x2 grid** to find mid **sized** objects and a **1x1 grid** to find **objects** that cover the entire image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDvflWuhVHSb"
      },
      "source": [
        "#### **SSD Advantages**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhlhVY3cVHSc"
      },
      "source": [
        "- SSD is a **single-shot detector**. \n",
        "\n",
        "- It has no delegated **region proposal** network and predicts the boundary boxes and the classes directly from feature maps in **one single pass**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHKK8f1sVHSc"
      },
      "source": [
        "- To **improve accuracy**, SSD introduces:\n",
        "\n",
        "  - **Small convolutional filters** to predict object classes and offsets to default boundary boxes.\n",
        "  \n",
        "  - **Separate filters** for default boxes to handle the difference in aspect ratios.\n",
        "  \n",
        "  - **Multi-scale feature maps** for object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nILcqwVHSd"
      },
      "source": [
        "- SSD can be trained **end-to-end** for better accuracy.\n",
        "\n",
        "- It makes more **predictions** and has a **better coverage** on location, scale and aspect ratios.\n",
        "\n",
        "- With the **improvements** above, It can **lower** the **input image resolution** to 300 × 300 with a comparative accuracy performance.\n",
        "\n",
        "- By removing the **delegated** region proposal and using lower **resolution** images, the model can **run at real-time speed** and still beats the accuracy of the **state-of-the-art** Faster R-CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byEVnNg6VHSe"
      },
      "source": [
        "---\n",
        "<a name = Section4></a>\n",
        "# **4. Conclusion**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHHx3FkBVHSe"
      },
      "source": [
        "- **Performance Comparison on COCO Object Detection Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDijXNCgVHSf"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/yolo_v3_performance.png\" width=\"900\" height=\"550\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_9nAx6UVHSg"
      },
      "source": [
        "- **YOLOv3** is an **updated** version of the YOLO architecture with some massive improvements.\n",
        "\n",
        "  - It is extremely **fast** and **accurate**.\n",
        "\n",
        "  - In mAP measured at .5 IOU **YOLOv3** is on par with **Focal Loss** but about **4x faster**.\n",
        "\n",
        "  - Moreover, you can easily **tradeoff** between speed and **accuracy** simply by **changing** the **size** of the model, no retraining required!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snrMGcDFVHSg"
      },
      "source": [
        "- From the plot above, we can see that **YOLOv3 outperforms SSD** in both speed and **accuracy**.\n",
        "\n",
        "- We will be studying **in-depth** about both these **architectures** next."
      ]
    }
  ]
}