{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lalitpandey02/PythonNotebooks/blob/main/WordLevel_LSTM0226.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qTZFFjJkugv"
      },
      "source": [
        "<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBO78ScK3FFx"
      },
      "source": [
        "<center><h1>Introduction to Word Level Language Modelling(Practical Implementation)</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGtLLB_ADRl_"
      },
      "source": [
        "---\n",
        "# **Table of Contents**\n",
        "---\n",
        "\n",
        "**1.** [**Introduction**](#Section1)<br>\n",
        "**2.** [**Problem Description**](#Section2)<br>\n",
        "**3.** [**Installing & Importing Libraries**](#Section3)<br>\n",
        "**4.** [**Data Acquisition & Description**](#Section4)<br>\n",
        "**5.** [**Data Preprocessing**](#Section5)<br>\n",
        "**6.** [**Train Language Model**](#Section6)<br>\n",
        "  - **6.1** [**Load Sequence**](#Section61)\n",
        "  - **6.2** [**Encode Sequence**](#Section62) \n",
        "  - **6.3** [**Sequence Inputs and Output**](#Section63)\n",
        "  - **6.4** [**Fit Model**](#Section61)\n",
        "\n",
        "**7.** [**Use Language Model**](#Section7)<br>\n",
        "  - **7.1** [**Load Sequence**](#Section61)\n",
        "  - **7.2** [**Load Model**](#Section62) \n",
        "  - **7.3** [**Fit Model**](#Section63)\n",
        "\n",
        "**8.** [**Conclusion**](#Section8)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt3g_Q2dZJwN"
      },
      "source": [
        "---\n",
        "<a name = Section1></a>\n",
        "# **1. Introduction**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dVcPtmpNiZJ"
      },
      "source": [
        "- **Language models** learn and **predict** one word at a time. The **training** of the network involves **providing** sequences of words as **input** that are processed one at a time where a **prediction** can be made and learned for each **input sequence**.\n",
        "\n",
        "- Neural Language Models (NLM) address the **N-gram data sparsity** issue through **parameterization** of words as **vectors** (word embeddings) and using them as inputs to a neural network.\n",
        "\n",
        "- Word **embeddings** obtained through NLMs **exhibit** the **property** whereby semantically close **words** are likewise **close** in the induced **vector space**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIpcO8AsTU1i"
      },
      "source": [
        "---\n",
        "<a name = Section2></a>\n",
        "# **2. Problem Statement**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B523lFJxHXT6"
      },
      "source": [
        "- The **problem statement** is to train a **language model** on the given text and then **generate** text given an input text in such a way that it looks **straight** out of this document and is **grammatically** correct and **legible** to read.\n",
        "\n",
        "* For this, we need to develop **word-level** neural language **model** and use  it to generate text.\n",
        "\n",
        "* A **language model** can predict the probability of the next word in the sequence, based on the **words** already **observed** in the sequence.\n",
        "\n",
        "* **Neural network models** are a preferred method for **developing statistical language models** because they can use a **distributed representation** where different words with similar meanings have **similar representation**.\n",
        "\n",
        "- Also, it is because they can use a **large context** of recently observed words when **making predictions**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOmVgY8yWE36"
      },
      "source": [
        "---\n",
        "<a name = Section3></a>\n",
        "# **3. Installing and Importing Libraries**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItkIWd-QuyJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123badeb-1241-4563-f891-1035013f6cd8"
      },
      "source": [
        "# Import tensorflow 2.x\n",
        "# This code block will only work in Google Colab.\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYgJy8N1Gpd2"
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import string"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9OWuHO0WF1n"
      },
      "source": [
        "---\n",
        "<a name = Section4></a>\n",
        "# **4. Data Acquisition & Description**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iIrOcT-Fn6N"
      },
      "source": [
        "- **The Republic by Plato**\n",
        "<br>\n",
        "<center> <img src=\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/socrates.JPG\" /></center>\n",
        "<br>\n",
        "\n",
        "-  The Republic is the **classical Greek philosopher Platoâ€™s** most famous work.\n",
        "\n",
        "- It is **structured** as a **dialog** (e.g. conversation) on the topic of **order and justice** within a city state\n",
        "\n",
        "- Download the ASCII **text version** of the entire book (or books) here: [The Republic](https://https://www.gutenberg.org/ebooks/1497) and save it as *republic.txt*\n",
        "\n",
        "- Open the file in a **text editor** and delete the **front** and **back** matter. \n",
        "\n",
        "- This includes details about the **book** at the beginning, a **long analysis**, and **license** information at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq0Ve7ItvsQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323d99c6-c303-483c-c536-c5b0f1361ad6"
      },
      "source": [
        "import urllib\n",
        "response = urllib.request.urlopen('https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/republic_clean.txt')\n",
        "doc = response.read().decode('utf8')\n",
        "print(doc[:800])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ï»¿\rBOOK I.\r\r\n",
            "\r\r\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\r\r\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\r\r\n",
            "Artemis.); and also because I wanted to see in what manner they would\r\r\n",
            "celebrate the festival, which was a new thing. I was delighted with the\r\r\n",
            "procession of the inhabitants; but that of the Thracians was equally,\r\r\n",
            "if not more, beautiful. When we had finished our prayers and viewed the\r\r\n",
            "spectacle, we turned in the direction of the city; and at that instant\r\r\n",
            "Polemarchus the son of Cephalus chanced to catch sight of us from a\r\r\n",
            "distance as we were starting on our way home, and told his servant to\r\r\n",
            "run and bid us wait for him. The servant took hold of me by the cloak\r\r\n",
            "behind, and said: Polemarchus desires you to wait.\r\r\n",
            "\r\r\n",
            "I turn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lszB-fmkWW4f"
      },
      "source": [
        "---\n",
        "<a name = Section5></a>\n",
        "# **5. Data Preprocessing**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvysmaT93FF6"
      },
      "source": [
        "We'll be using the following **process sequence** in this notebook:\n",
        "\n",
        "<br>   \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow0.png\"width=\"600\" height=\"400\"/></center>\n",
        "\n",
        "<br>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKSbwDDO3FGU"
      },
      "source": [
        "\n",
        "#### Clean Text\n",
        "\n",
        "* **Replace â€˜â€“â€˜** with a white space so we can split words better.\n",
        "\n",
        "* **Split words** based on **white space**.\n",
        "\n",
        "* Remove all **punctuation** from **words** to reduce the vocabulary size (e.g. â€˜What?â€™ becomes â€˜Whatâ€™).\n",
        "\n",
        "* **Remove all words** that are not alphabetic to remove standalone **punctuation tokens**.\n",
        "\n",
        "* Normalize **all words** to **lowercase** to reduce the **vocabulary size**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCORHQHPGmTC"
      },
      "source": [
        "\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hM6CnI1qOBdE",
        "outputId": "07bcc6ad-5a18-47d2-ecd2-4f923bd66e9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPb4XvVCGslQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373aa6c4-806d-4f6d-e0dd-429adc71af97"
      },
      "source": [
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOI6sfwaG8QB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14c5b75-2cc5-4d2e-cadc-08bbc99a630a"
      },
      "source": [
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "    # select sequence of tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # convert into a line\n",
        "    line = ' '.join(seq)\n",
        "    # store\n",
        "    sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 118633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5YCAlga3FGw",
        "outputId": "50514f89-466c-4799-e228-e4ca7c4954b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sequences[:5]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted',\n",
              " 'i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with',\n",
              " 'went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the',\n",
              " 'down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf7dLooF3FGp"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- Transforming the tokens into **space-separated strings** for later storage in a file.\n",
        "\n",
        "- Splitting the list of **clean tokens** into **sequences**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RksKge0_G_Oj"
      },
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "    \n",
        "# save sequences to file\n",
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IwlwV4_H3OW"
      },
      "source": [
        "----\n",
        "<a id=section6></a>\n",
        "## **6. Train Language Model**\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDXpKVrv3FG6"
      },
      "source": [
        "\n",
        "<br>   \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow4.png\"width=\"700\" height=\"400\"/></center>\n",
        "\n",
        "<br>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHCFMESq3FG7"
      },
      "source": [
        "* Model uses a **distributed** representation for words so that different words with similar meanings will have a similar representation.\n",
        "\n",
        "* It **learns** the **representation** at the same time as **learning the model.**\n",
        "\n",
        "* It **learns** to predict the **probability** for the next **word** using the **context** of the last **100 words**.\n",
        "\n",
        "- We will use an **Embedding Layer** to learn the representation of words, and a **Long Short-Term Memory (LSTM)** recurrent neural network to learn to **predict words** based on their context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NmWD7bb3FG_"
      },
      "source": [
        "<a id=section601></a>\n",
        "### **6.1 Load Sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBgoos1p3FHB"
      },
      "source": [
        "- We can load our **training data** using the **`load_doc()`** function defined below.\n",
        "\n",
        "\n",
        "- Once loaded, we can **split the data into separate training sequences** by splitting based on new lines.\n",
        "\n",
        "\n",
        "- The snippet below will load the **â€˜republic_sequences.txtâ€˜** data file from the current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYZIaBv-IRDJ"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7kz_wjL3FHH",
        "outputId": "cd0c9723-3bd7-4191-a1ad-dc9ed9738ffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lines[:2]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhFPp4lxIW1E"
      },
      "source": [
        "<a id=section602></a>\n",
        "### **6.2 Encode Sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdcRYdZI3FHN"
      },
      "source": [
        "- The **word embedding layer** expects input sequences to be comprised of integers.\n",
        "\n",
        "- We can **map each word in our vocabulary** to a unique integer and encode our input sequences.\n",
        "\n",
        "- Later, when we make predictions, we can convert the **prediction to numbers** and look up their **associated words** in the **same mapping**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDn6Si02IeI6"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "First, the Tokenizer must be trained on the entire training dataset, which means it finds all of the unique words in the data and assigns each a unique integer.\n",
        "\n",
        "We can then use the fit Tokenizer to encode all of the training sequences, converting each sequence from a list of words to a list of integers.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxR_DnR9I7Zt"
      },
      "source": [
        "- We can access the **mapping** of **words** to **integers** as a dictionary attribute called **`word_index`** on the **tokenizer** object.\n",
        "\n",
        "- We need to know the **size** of the **vocabulary** for defining the **embedding** layer later. \n",
        "\n",
        "- We can determine the vocabulary by **calculating** the size of the **mapping dictionary**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2ZZJ3DxI_2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65569113-efaa-4157-9aae-34f12d736b60"
      },
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7410"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5HavXQBXqUu",
        "outputId": "6fae52c6-8897-45db-a7d6-4f1419ce88f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'and': 2,\n",
              " 'of': 3,\n",
              " 'to': 4,\n",
              " 'is': 5,\n",
              " 'in': 6,\n",
              " 'he': 7,\n",
              " 'a': 8,\n",
              " 'that': 9,\n",
              " 'be': 10,\n",
              " 'i': 11,\n",
              " 'not': 12,\n",
              " 'which': 13,\n",
              " 'are': 14,\n",
              " 'you': 15,\n",
              " 'they': 16,\n",
              " 'or': 17,\n",
              " 'will': 18,\n",
              " 'said': 19,\n",
              " 'as': 20,\n",
              " 'we': 21,\n",
              " 'but': 22,\n",
              " 'have': 23,\n",
              " 'them': 24,\n",
              " 'his': 25,\n",
              " 'for': 26,\n",
              " 'by': 27,\n",
              " 'who': 28,\n",
              " 'their': 29,\n",
              " 'what': 30,\n",
              " 'then': 31,\n",
              " 'this': 32,\n",
              " 'one': 33,\n",
              " 'if': 34,\n",
              " 'with': 35,\n",
              " 'there': 36,\n",
              " 'all': 37,\n",
              " 'true': 38,\n",
              " 'at': 39,\n",
              " 'when': 40,\n",
              " 'do': 41,\n",
              " 'other': 42,\n",
              " 'has': 43,\n",
              " 'yes': 44,\n",
              " 'any': 45,\n",
              " 'him': 46,\n",
              " 'no': 47,\n",
              " 'good': 48,\n",
              " 'would': 49,\n",
              " 'may': 50,\n",
              " 'state': 51,\n",
              " 'from': 52,\n",
              " 'man': 53,\n",
              " 'say': 54,\n",
              " 'our': 55,\n",
              " 'only': 56,\n",
              " 'was': 57,\n",
              " 'an': 58,\n",
              " 'must': 59,\n",
              " 'should': 60,\n",
              " 'so': 61,\n",
              " 'more': 62,\n",
              " 'us': 63,\n",
              " 'can': 64,\n",
              " 'on': 65,\n",
              " 'were': 66,\n",
              " 'very': 67,\n",
              " 'now': 68,\n",
              " 'like': 69,\n",
              " 'such': 70,\n",
              " 'replied': 71,\n",
              " 'just': 72,\n",
              " 'certainly': 73,\n",
              " 'than': 74,\n",
              " 'also': 75,\n",
              " 'these': 76,\n",
              " 'men': 77,\n",
              " 'same': 78,\n",
              " 'another': 79,\n",
              " 'about': 80,\n",
              " 'justice': 81,\n",
              " 'own': 82,\n",
              " 'how': 83,\n",
              " 'soul': 84,\n",
              " 'been': 85,\n",
              " 'let': 86,\n",
              " 'into': 87,\n",
              " 'being': 88,\n",
              " 'shall': 89,\n",
              " 'it': 90,\n",
              " 'most': 91,\n",
              " 'my': 92,\n",
              " 'me': 93,\n",
              " 'nature': 94,\n",
              " 'whether': 95,\n",
              " 'life': 96,\n",
              " 'had': 97,\n",
              " 'many': 98,\n",
              " 'those': 99,\n",
              " 'things': 100,\n",
              " 'some': 101,\n",
              " 'way': 102,\n",
              " 'mean': 103,\n",
              " 'knowledge': 104,\n",
              " 'well': 105,\n",
              " 'first': 106,\n",
              " 'your': 107,\n",
              " 'make': 108,\n",
              " 'know': 109,\n",
              " 'see': 110,\n",
              " 'her': 111,\n",
              " 'out': 112,\n",
              " 'think': 113,\n",
              " 'evil': 114,\n",
              " 'right': 115,\n",
              " 'truth': 116,\n",
              " 'himself': 117,\n",
              " 'whom': 118,\n",
              " 'sort': 119,\n",
              " 'because': 120,\n",
              " 'quite': 121,\n",
              " 'never': 122,\n",
              " 'up': 123,\n",
              " 'unjust': 124,\n",
              " 'better': 125,\n",
              " 'injustice': 126,\n",
              " 'reason': 127,\n",
              " 'others': 128,\n",
              " 'saying': 129,\n",
              " 'either': 130,\n",
              " 'therefore': 131,\n",
              " 'nothing': 132,\n",
              " 'every': 133,\n",
              " 'am': 134,\n",
              " 'two': 135,\n",
              " 'great': 136,\n",
              " 'best': 137,\n",
              " 'opinion': 138,\n",
              " 'city': 139,\n",
              " 'suppose': 140,\n",
              " 'far': 141,\n",
              " 'take': 142,\n",
              " 'ought': 143,\n",
              " 'having': 144,\n",
              " 'both': 145,\n",
              " 'question': 146,\n",
              " 'time': 147,\n",
              " 'made': 148,\n",
              " 'upon': 149,\n",
              " 'why': 150,\n",
              " 'thing': 151,\n",
              " 'cannot': 152,\n",
              " 'art': 153,\n",
              " 'again': 154,\n",
              " 'part': 155,\n",
              " 'order': 156,\n",
              " 'gods': 157,\n",
              " 'after': 158,\n",
              " 'mind': 159,\n",
              " 'yet': 160,\n",
              " 'does': 161,\n",
              " 'nor': 162,\n",
              " 'power': 163,\n",
              " 'too': 164,\n",
              " 'neither': 165,\n",
              " 'able': 166,\n",
              " 'virtue': 167,\n",
              " 'manner': 168,\n",
              " 'whole': 169,\n",
              " 'much': 170,\n",
              " 'body': 171,\n",
              " 'use': 172,\n",
              " 'always': 173,\n",
              " 'themselves': 174,\n",
              " 'guardians': 175,\n",
              " 'go': 176,\n",
              " 'even': 177,\n",
              " 'principle': 178,\n",
              " 'might': 179,\n",
              " 'under': 180,\n",
              " 'tell': 181,\n",
              " 'friend': 182,\n",
              " 'still': 183,\n",
              " 'words': 184,\n",
              " 'rather': 185,\n",
              " 'anything': 186,\n",
              " 'pleasure': 187,\n",
              " 'each': 188,\n",
              " 'ever': 189,\n",
              " 'rulers': 190,\n",
              " 'she': 191,\n",
              " 'ask': 192,\n",
              " 'answer': 193,\n",
              " 'before': 194,\n",
              " 'speaking': 195,\n",
              " 'over': 196,\n",
              " 'clearly': 197,\n",
              " 'socrates': 198,\n",
              " 'come': 199,\n",
              " 'greatest': 200,\n",
              " 'place': 201,\n",
              " 'against': 202,\n",
              " 'class': 203,\n",
              " 'children': 204,\n",
              " 'case': 205,\n",
              " 'further': 206,\n",
              " 'agree': 207,\n",
              " 'among': 208,\n",
              " 'found': 209,\n",
              " 'world': 210,\n",
              " 'kind': 211,\n",
              " 'citizens': 212,\n",
              " 'glaucon': 213,\n",
              " 'light': 214,\n",
              " 'likely': 215,\n",
              " 'pleasures': 216,\n",
              " 'greater': 217,\n",
              " 'god': 218,\n",
              " 'honour': 219,\n",
              " 'desire': 220,\n",
              " 'give': 221,\n",
              " 'hear': 222,\n",
              " 'indeed': 223,\n",
              " 'philosophy': 224,\n",
              " 'thus': 225,\n",
              " 'sight': 226,\n",
              " 'youth': 227,\n",
              " 'less': 228,\n",
              " 'consider': 229,\n",
              " 'view': 230,\n",
              " 'did': 231,\n",
              " 'war': 232,\n",
              " 'desires': 233,\n",
              " 'away': 234,\n",
              " 'end': 235,\n",
              " 'without': 236,\n",
              " 'call': 237,\n",
              " 'money': 238,\n",
              " 'argument': 239,\n",
              " 'interest': 240,\n",
              " 'three': 241,\n",
              " 'wisdom': 242,\n",
              " 'women': 243,\n",
              " 'law': 244,\n",
              " 'education': 245,\n",
              " 'old': 246,\n",
              " 'look': 247,\n",
              " 'believe': 248,\n",
              " 'understand': 249,\n",
              " 'states': 250,\n",
              " 'spirit': 251,\n",
              " 'thrasymachus': 252,\n",
              " 'seen': 253,\n",
              " 'people': 254,\n",
              " 'imagine': 255,\n",
              " 'little': 256,\n",
              " 'else': 257,\n",
              " 'human': 258,\n",
              " 'enough': 259,\n",
              " 'while': 260,\n",
              " 'son': 261,\n",
              " 'where': 262,\n",
              " 'speak': 263,\n",
              " 'could': 264,\n",
              " 'wise': 265,\n",
              " 'love': 266,\n",
              " 'bad': 267,\n",
              " 'point': 268,\n",
              " 'persons': 269,\n",
              " 'thought': 270,\n",
              " 'natural': 271,\n",
              " 'care': 272,\n",
              " 'form': 273,\n",
              " 'work': 274,\n",
              " 'natures': 275,\n",
              " 'friends': 276,\n",
              " 'want': 277,\n",
              " 'rest': 278,\n",
              " 'become': 279,\n",
              " 'music': 280,\n",
              " 'tyrant': 281,\n",
              " 'young': 282,\n",
              " 'opposite': 283,\n",
              " 'truly': 284,\n",
              " 'number': 285,\n",
              " 'arts': 286,\n",
              " 'next': 287,\n",
              " 'together': 288,\n",
              " 'making': 289,\n",
              " 'person': 290,\n",
              " 'find': 291,\n",
              " 'proceed': 292,\n",
              " 'course': 293,\n",
              " 'present': 294,\n",
              " 'individual': 295,\n",
              " 'different': 296,\n",
              " 'common': 297,\n",
              " 'father': 298,\n",
              " 'remember': 299,\n",
              " 'surely': 300,\n",
              " 'homer': 301,\n",
              " 'put': 302,\n",
              " 'really': 303,\n",
              " 'government': 304,\n",
              " 'called': 305,\n",
              " 'eyes': 306,\n",
              " 'last': 307,\n",
              " 'pain': 308,\n",
              " 'sense': 309,\n",
              " 'fear': 310,\n",
              " 'given': 311,\n",
              " 'necessity': 312,\n",
              " 'rule': 313,\n",
              " 'although': 314,\n",
              " 'certain': 315,\n",
              " 'until': 316,\n",
              " 'already': 317,\n",
              " 'qualities': 318,\n",
              " 'third': 319,\n",
              " 'comes': 320,\n",
              " 'beauty': 321,\n",
              " 'done': 322,\n",
              " 'matter': 323,\n",
              " 'hand': 324,\n",
              " 'subject': 325,\n",
              " 'private': 326,\n",
              " 'turn': 327,\n",
              " 'change': 328,\n",
              " 'down': 329,\n",
              " 'between': 330,\n",
              " 'meaning': 331,\n",
              " 'enemies': 332,\n",
              " 'need': 333,\n",
              " 'subjects': 334,\n",
              " 'help': 335,\n",
              " 'adeimantus': 336,\n",
              " 'age': 337,\n",
              " 'drink': 338,\n",
              " 'general': 339,\n",
              " 'name': 340,\n",
              " 'makes': 341,\n",
              " 'idea': 342,\n",
              " 'within': 343,\n",
              " 'here': 344,\n",
              " 'equally': 345,\n",
              " 'stronger': 346,\n",
              " 'says': 347,\n",
              " 'rich': 348,\n",
              " 'whose': 349,\n",
              " 'word': 350,\n",
              " 'means': 351,\n",
              " 'lover': 352,\n",
              " 'real': 353,\n",
              " 'absolute': 354,\n",
              " 'temperance': 355,\n",
              " 'something': 356,\n",
              " 'doing': 357,\n",
              " 'sure': 358,\n",
              " 'though': 359,\n",
              " 'small': 360,\n",
              " 'principles': 361,\n",
              " 'difficulty': 362,\n",
              " 'death': 363,\n",
              " 'similar': 364,\n",
              " 'philosopher': 365,\n",
              " 'appear': 366,\n",
              " 'laws': 367,\n",
              " 'ruler': 368,\n",
              " 'whereas': 369,\n",
              " 'character': 370,\n",
              " 'hardly': 371,\n",
              " 'once': 372,\n",
              " 'heaven': 373,\n",
              " 'exactly': 374,\n",
              " 'begin': 375,\n",
              " 'gymnastic': 376,\n",
              " 'guardian': 377,\n",
              " 'receive': 378,\n",
              " 'lives': 379,\n",
              " 'impossible': 380,\n",
              " 'public': 381,\n",
              " 'live': 382,\n",
              " 'existence': 383,\n",
              " 'possible': 384,\n",
              " 'asked': 385,\n",
              " 'going': 386,\n",
              " 'often': 387,\n",
              " 'poets': 388,\n",
              " 'longer': 389,\n",
              " 'wealth': 390,\n",
              " 'second': 391,\n",
              " 'appears': 392,\n",
              " 'doubt': 393,\n",
              " 'instead': 394,\n",
              " 'hands': 395,\n",
              " 'eye': 396,\n",
              " 'forms': 397,\n",
              " 'science': 398,\n",
              " 'happiness': 399,\n",
              " 'worse': 400,\n",
              " 'full': 401,\n",
              " 'get': 402,\n",
              " 'lie': 403,\n",
              " 'example': 404,\n",
              " 'unless': 405,\n",
              " 'perfectly': 406,\n",
              " 'perfect': 407,\n",
              " 'day': 408,\n",
              " 'difference': 409,\n",
              " 'courage': 410,\n",
              " 'follow': 411,\n",
              " 'grow': 412,\n",
              " 'study': 413,\n",
              " 'years': 414,\n",
              " 'long': 415,\n",
              " 'medicine': 416,\n",
              " 'side': 417,\n",
              " 'taken': 418,\n",
              " 'none': 419,\n",
              " 'supposed': 420,\n",
              " 'allow': 421,\n",
              " 'pure': 422,\n",
              " 'simple': 423,\n",
              " 'object': 424,\n",
              " 'brought': 425,\n",
              " 'earth': 426,\n",
              " 'noble': 427,\n",
              " 'higher': 428,\n",
              " 'necessary': 429,\n",
              " 'beautiful': 430,\n",
              " 'told': 431,\n",
              " 'few': 432,\n",
              " 'wrong': 433,\n",
              " 'seem': 434,\n",
              " 'however': 435,\n",
              " 'according': 436,\n",
              " 'off': 437,\n",
              " 'gain': 438,\n",
              " 'health': 439,\n",
              " 'business': 440,\n",
              " 'philosophers': 441,\n",
              " 'divine': 442,\n",
              " 'imitation': 443,\n",
              " 'democracy': 444,\n",
              " 'towards': 445,\n",
              " 'poor': 446,\n",
              " 'gold': 447,\n",
              " 'enemy': 448,\n",
              " 'allowed': 449,\n",
              " 'above': 450,\n",
              " 'ready': 451,\n",
              " 'times': 452,\n",
              " 'four': 453,\n",
              " 'learn': 454,\n",
              " 'knows': 455,\n",
              " 'enquiry': 456,\n",
              " 'ignorance': 457,\n",
              " 'harmony': 458,\n",
              " 'sun': 459,\n",
              " 'fair': 460,\n",
              " 'experience': 461,\n",
              " 'poet': 462,\n",
              " 'sons': 463,\n",
              " 'reverse': 464,\n",
              " 'proper': 465,\n",
              " 'food': 466,\n",
              " 'dear': 467,\n",
              " 'came': 468,\n",
              " 'quality': 469,\n",
              " 'seeing': 470,\n",
              " 'highest': 471,\n",
              " 'described': 472,\n",
              " 'degree': 473,\n",
              " 'constitution': 474,\n",
              " 'master': 475,\n",
              " 'several': 476,\n",
              " 'happy': 477,\n",
              " 'termed': 478,\n",
              " 'wish': 479,\n",
              " 'admit': 480,\n",
              " 'required': 481,\n",
              " 'compelled': 482,\n",
              " 'vice': 483,\n",
              " 'through': 484,\n",
              " 'beyond': 485,\n",
              " 'individuals': 486,\n",
              " 'image': 487,\n",
              " 'ridiculous': 488,\n",
              " 'hold': 489,\n",
              " 'style': 490,\n",
              " 'becomes': 491,\n",
              " 'bodily': 492,\n",
              " 'term': 493,\n",
              " 'mankind': 494,\n",
              " 'least': 495,\n",
              " 'conceive': 496,\n",
              " 'set': 497,\n",
              " 'beginning': 498,\n",
              " 'show': 499,\n",
              " 'objects': 500,\n",
              " 'describing': 501,\n",
              " 'lovers': 502,\n",
              " 'polemarchus': 503,\n",
              " 'regard': 504,\n",
              " 'evils': 505,\n",
              " 'easily': 506,\n",
              " 'condition': 507,\n",
              " 'physician': 508,\n",
              " 'harm': 509,\n",
              " 'thinking': 510,\n",
              " 'reality': 511,\n",
              " 'admitted': 512,\n",
              " 'observe': 513,\n",
              " 'perhaps': 514,\n",
              " 'ways': 515,\n",
              " 'particular': 516,\n",
              " 'souls': 517,\n",
              " 'poetry': 518,\n",
              " 'imitate': 519,\n",
              " 'faculty': 520,\n",
              " 'shadows': 521,\n",
              " 'satisfied': 522,\n",
              " 'parents': 523,\n",
              " 'disease': 524,\n",
              " 'assuredly': 525,\n",
              " 'heard': 526,\n",
              " 'rightly': 527,\n",
              " 'force': 528,\n",
              " 'miserable': 529,\n",
              " 'house': 530,\n",
              " 'company': 531,\n",
              " 'property': 532,\n",
              " 'below': 533,\n",
              " 'gives': 534,\n",
              " 'result': 535,\n",
              " 'latter': 536,\n",
              " 'nay': 537,\n",
              " 'ignorant': 538,\n",
              " 'generally': 539,\n",
              " 'advantage': 540,\n",
              " 'try': 541,\n",
              " 'strength': 542,\n",
              " 'understanding': 543,\n",
              " 'pains': 544,\n",
              " 'ones': 545,\n",
              " 'maintain': 546,\n",
              " 'unable': 547,\n",
              " 'oligarchy': 548,\n",
              " 'new': 549,\n",
              " 'pass': 550,\n",
              " 'since': 551,\n",
              " 'keep': 552,\n",
              " 'easy': 553,\n",
              " 'cause': 554,\n",
              " 'peace': 555,\n",
              " 'clear': 556,\n",
              " 'act': 557,\n",
              " 'seeking': 558,\n",
              " 'yourself': 559,\n",
              " 'whatever': 560,\n",
              " 'taking': 561,\n",
              " 'authority': 562,\n",
              " 'behold': 563,\n",
              " 'learning': 564,\n",
              " 'praise': 565,\n",
              " 'everything': 566,\n",
              " 'concerned': 567,\n",
              " 'action': 568,\n",
              " 'left': 569,\n",
              " 'discovered': 570,\n",
              " 'classes': 571,\n",
              " 'effect': 572,\n",
              " 'former': 573,\n",
              " 'bring': 574,\n",
              " 'habit': 575,\n",
              " 'woman': 576,\n",
              " 'geometry': 577,\n",
              " 'turned': 578,\n",
              " 'known': 579,\n",
              " 'draw': 580,\n",
              " 'sake': 581,\n",
              " 'useful': 582,\n",
              " 'knowing': 583,\n",
              " 'silver': 584,\n",
              " 'useless': 585,\n",
              " 'agreed': 586,\n",
              " 'danger': 587,\n",
              " 'notion': 588,\n",
              " 'goes': 589,\n",
              " 'tyrannical': 590,\n",
              " 'maker': 591,\n",
              " 'takes': 592,\n",
              " 'slaves': 593,\n",
              " 'single': 594,\n",
              " 'houses': 595,\n",
              " 'kinds': 596,\n",
              " 'animals': 597,\n",
              " 'sorrow': 598,\n",
              " 'passion': 599,\n",
              " 'intermediate': 600,\n",
              " 'visible': 601,\n",
              " 'oligarchical': 602,\n",
              " 'bed': 603,\n",
              " 'round': 604,\n",
              " 'deny': 605,\n",
              " 'freedom': 606,\n",
              " 'country': 607,\n",
              " 'leave': 608,\n",
              " 'actions': 609,\n",
              " 'shown': 610,\n",
              " 'saw': 611,\n",
              " 'escape': 612,\n",
              " 'follows': 613,\n",
              " 'inferior': 614,\n",
              " 'require': 615,\n",
              " 'alone': 616,\n",
              " 'tyranny': 617,\n",
              " 'excellence': 618,\n",
              " 'false': 619,\n",
              " 'minds': 620,\n",
              " 'days': 621,\n",
              " 'upwards': 622,\n",
              " 'trained': 623,\n",
              " 'moment': 624,\n",
              " 'rhythm': 625,\n",
              " 'training': 626,\n",
              " 'element': 627,\n",
              " 'relation': 628,\n",
              " 'share': 629,\n",
              " 'pursuits': 630,\n",
              " 'gifts': 631,\n",
              " 'perceive': 632,\n",
              " 'tale': 633,\n",
              " 'bear': 634,\n",
              " 'pilot': 635,\n",
              " 'excellent': 636,\n",
              " 'seems': 637,\n",
              " 'angry': 638,\n",
              " 'ruling': 639,\n",
              " 'acknowledge': 640,\n",
              " 'mans': 641,\n",
              " 'deemed': 642,\n",
              " 'judge': 643,\n",
              " 'fight': 644,\n",
              " 'assume': 645,\n",
              " 'utterly': 646,\n",
              " 'middle': 647,\n",
              " 'strain': 648,\n",
              " 'sorts': 649,\n",
              " 'educated': 650,\n",
              " 'influence': 651,\n",
              " 'naturally': 652,\n",
              " 'spoke': 653,\n",
              " 'images': 654,\n",
              " 'home': 655,\n",
              " 'enquire': 656,\n",
              " 'feeling': 657,\n",
              " 'near': 658,\n",
              " 'tales': 659,\n",
              " 'pay': 660,\n",
              " 'back': 661,\n",
              " 'spoken': 662,\n",
              " 'alike': 663,\n",
              " 'command': 664,\n",
              " 'play': 665,\n",
              " 'account': 666,\n",
              " 'ill': 667,\n",
              " 'itself': 668,\n",
              " 'names': 669,\n",
              " 'whenever': 670,\n",
              " 'worst': 671,\n",
              " 'affirm': 672,\n",
              " 'ground': 673,\n",
              " 'suffer': 674,\n",
              " 'judgment': 675,\n",
              " 'honours': 676,\n",
              " 'describe': 677,\n",
              " 'mother': 678,\n",
              " 'painter': 679,\n",
              " 'original': 680,\n",
              " 'ourselves': 681,\n",
              " 'sees': 682,\n",
              " 'zeus': 683,\n",
              " 'parts': 684,\n",
              " 'military': 685,\n",
              " 'possibility': 686,\n",
              " 'virtues': 687,\n",
              " 'sciences': 688,\n",
              " 'received': 689,\n",
              " 'thinks': 690,\n",
              " 'child': 691,\n",
              " 'attempt': 692,\n",
              " 'duty': 693,\n",
              " 'interests': 694,\n",
              " 'sometimes': 695,\n",
              " 'mistaken': 696,\n",
              " 'respect': 697,\n",
              " 'fails': 698,\n",
              " 'its': 699,\n",
              " 'office': 700,\n",
              " 'free': 701,\n",
              " 'considered': 702,\n",
              " 'honourable': 703,\n",
              " 'carried': 704,\n",
              " 'lest': 705,\n",
              " 'gentle': 706,\n",
              " 'appearance': 707,\n",
              " 'causes': 708,\n",
              " 'dog': 709,\n",
              " 'parent': 710,\n",
              " 'attain': 711,\n",
              " 'dialectic': 712,\n",
              " 'run': 713,\n",
              " 'arrived': 714,\n",
              " 'sweet': 715,\n",
              " 'undoubtedly': 716,\n",
              " 'dogs': 717,\n",
              " 'battle': 718,\n",
              " 'discussion': 719,\n",
              " 'willing': 720,\n",
              " 'fact': 721,\n",
              " 'numbers': 722,\n",
              " 'cases': 723,\n",
              " 'payment': 724,\n",
              " 'utter': 725,\n",
              " 'conclusion': 726,\n",
              " 'discover': 727,\n",
              " 'mere': 728,\n",
              " 'wants': 729,\n",
              " 'origin': 730,\n",
              " 'hearing': 731,\n",
              " 'remains': 732,\n",
              " 'large': 733,\n",
              " 'reasons': 734,\n",
              " 'unlike': 735,\n",
              " 'please': 736,\n",
              " 'add': 737,\n",
              " 'purpose': 738,\n",
              " 'rewards': 739,\n",
              " 'liberty': 740,\n",
              " 'king': 741,\n",
              " 'dead': 742,\n",
              " 'finger': 743,\n",
              " 'cities': 744,\n",
              " 'process': 745,\n",
              " 'land': 746,\n",
              " 'especially': 747,\n",
              " 'author': 748,\n",
              " 'matters': 749,\n",
              " 'previous': 750,\n",
              " 'lot': 751,\n",
              " 'meet': 752,\n",
              " 'harmonies': 753,\n",
              " 'drawn': 754,\n",
              " 'equal': 755,\n",
              " 'intelligence': 756,\n",
              " 'proportion': 757,\n",
              " 'hellenes': 758,\n",
              " 'measure': 759,\n",
              " 'thirst': 760,\n",
              " 'division': 761,\n",
              " 'pursuit': 762,\n",
              " 'distance': 763,\n",
              " 'coming': 764,\n",
              " 'listen': 765,\n",
              " 'soon': 766,\n",
              " 'journey': 767,\n",
              " 'fault': 768,\n",
              " 'answered': 769,\n",
              " 'possess': 770,\n",
              " 'profit': 771,\n",
              " 'possession': 772,\n",
              " 'sacrifices': 773,\n",
              " 'probably': 774,\n",
              " 'giving': 775,\n",
              " 'guilty': 776,\n",
              " 'mistake': 777,\n",
              " 'myself': 778,\n",
              " 'weaker': 779,\n",
              " 'absolutely': 780,\n",
              " 'liable': 781,\n",
              " 'contrary': 782,\n",
              " 'fail': 783,\n",
              " 'deal': 784,\n",
              " 'got': 785,\n",
              " 'entirely': 786,\n",
              " 'ideas': 787,\n",
              " 'getting': 788,\n",
              " 'ears': 789,\n",
              " 'choose': 790,\n",
              " 'incapable': 791,\n",
              " 'unity': 792,\n",
              " 'explain': 793,\n",
              " 'necessarily': 794,\n",
              " 'practise': 795,\n",
              " 'arise': 796,\n",
              " 'lead': 797,\n",
              " 'along': 798,\n",
              " 'continue': 799,\n",
              " 'fourth': 800,\n",
              " 'water': 801,\n",
              " 'husbandman': 802,\n",
              " 'motion': 803,\n",
              " 'destroy': 804,\n",
              " 'elements': 805,\n",
              " 'aware': 806,\n",
              " 'imitator': 807,\n",
              " 'fall': 808,\n",
              " 'soldiers': 809,\n",
              " 'strange': 810,\n",
              " 'sphere': 811,\n",
              " 'fathers': 812,\n",
              " 'terms': 813,\n",
              " 'essence': 814,\n",
              " 'wives': 815,\n",
              " 'notbeing': 816,\n",
              " 'offer': 817,\n",
              " 'took': 818,\n",
              " 'persuade': 819,\n",
              " 'carry': 820,\n",
              " 'head': 821,\n",
              " 'difficult': 822,\n",
              " 'feel': 823,\n",
              " 'fortune': 824,\n",
              " 'hence': 825,\n",
              " 'begins': 826,\n",
              " 'hope': 827,\n",
              " 'occasion': 828,\n",
              " 'greatly': 829,\n",
              " 'advantages': 830,\n",
              " 'o': 831,\n",
              " 'meant': 832,\n",
              " 'musician': 833,\n",
              " 'benefit': 834,\n",
              " 'clearness': 835,\n",
              " 'hard': 836,\n",
              " 'answers': 837,\n",
              " 'dare': 838,\n",
              " 'notions': 839,\n",
              " 'acknowledged': 840,\n",
              " 'artist': 841,\n",
              " 'distinguished': 842,\n",
              " 'blessed': 843,\n",
              " 'fairly': 844,\n",
              " 'passed': 845,\n",
              " 'proof': 846,\n",
              " 'began': 847,\n",
              " 'special': 848,\n",
              " 'laid': 849,\n",
              " 'trouble': 850,\n",
              " 'charming': 851,\n",
              " 'profitable': 852,\n",
              " 'rate': 853,\n",
              " 'creature': 854,\n",
              " 'exist': 855,\n",
              " 'army': 856,\n",
              " 'appointed': 857,\n",
              " 'lost': 858,\n",
              " 'apply': 859,\n",
              " 'taste': 860,\n",
              " 'voice': 861,\n",
              " 'honoured': 862,\n",
              " 'service': 863,\n",
              " 'wonder': 864,\n",
              " 'likeness': 865,\n",
              " 'dream': 866,\n",
              " 'vision': 867,\n",
              " 'courageous': 868,\n",
              " 'carpenter': 869,\n",
              " 'kings': 870,\n",
              " 'slave': 871,\n",
              " 'nearly': 872,\n",
              " 'discord': 873,\n",
              " 'corruption': 874,\n",
              " 'passionate': 875,\n",
              " 'ruin': 876,\n",
              " 'upper': 877,\n",
              " 'community': 878,\n",
              " 'figure': 879,\n",
              " 'appeared': 880,\n",
              " 'remain': 881,\n",
              " 'refuse': 882,\n",
              " 'horses': 883,\n",
              " 'convinced': 884,\n",
              " 'famous': 885,\n",
              " 'reply': 886,\n",
              " 'reflect': 887,\n",
              " 'arms': 888,\n",
              " 'return': 889,\n",
              " 'senses': 890,\n",
              " 'bodies': 891,\n",
              " 'guard': 892,\n",
              " 'expected': 893,\n",
              " 'wild': 894,\n",
              " 'beast': 895,\n",
              " 'fixed': 896,\n",
              " 'length': 897,\n",
              " 'wholly': 898,\n",
              " 'democratical': 899,\n",
              " 'remark': 900,\n",
              " 'used': 901,\n",
              " 'skill': 902,\n",
              " 'questions': 903,\n",
              " 'future': 904,\n",
              " 'superior': 905,\n",
              " 'provide': 906,\n",
              " 'aim': 907,\n",
              " 'perfection': 908,\n",
              " 'fancy': 909,\n",
              " 'learned': 910,\n",
              " 'determine': 911,\n",
              " 'lesser': 912,\n",
              " 'assent': 913,\n",
              " 'various': 914,\n",
              " 'held': 915,\n",
              " 'knew': 916,\n",
              " 'serious': 917,\n",
              " 'goods': 918,\n",
              " 'judges': 919,\n",
              " 'strong': 920,\n",
              " 'earnest': 921,\n",
              " 'otherwise': 922,\n",
              " 'choice': 923,\n",
              " 'relative': 924,\n",
              " 'quarrel': 925,\n",
              " 'family': 926,\n",
              " 'except': 927,\n",
              " 'blind': 928,\n",
              " 'misery': 929,\n",
              " 'passing': 930,\n",
              " 'reputation': 931,\n",
              " 'worthy': 932,\n",
              " 'chosen': 933,\n",
              " 'becoming': 934,\n",
              " 'heavens': 935,\n",
              " 'forth': 936,\n",
              " 'living': 937,\n",
              " 'numerous': 938,\n",
              " 'heroes': 939,\n",
              " 'afraid': 940,\n",
              " 'offspring': 941,\n",
              " 'larger': 942,\n",
              " 'mention': 943,\n",
              " 'labour': 944,\n",
              " 'multitude': 945,\n",
              " 'warrior': 946,\n",
              " 'duties': 947,\n",
              " 'spirited': 948,\n",
              " 'gymnastics': 949,\n",
              " 'external': 950,\n",
              " 'circumstances': 951,\n",
              " 'fairest': 952,\n",
              " 'temperate': 953,\n",
              " 'habits': 954,\n",
              " 'asclepius': 955,\n",
              " 'happen': 956,\n",
              " 'ideal': 957,\n",
              " 'holds': 958,\n",
              " 'legislator': 959,\n",
              " 'appetites': 960,\n",
              " 'akin': 961,\n",
              " 'birth': 962,\n",
              " 'hypotheses': 963,\n",
              " 'remaining': 964,\n",
              " 'direction': 965,\n",
              " 'servant': 966,\n",
              " 'brother': 967,\n",
              " 'commonly': 968,\n",
              " 'suspect': 969,\n",
              " 'besides': 970,\n",
              " 'expect': 971,\n",
              " 'sum': 972,\n",
              " 'sleep': 973,\n",
              " 'debt': 974,\n",
              " 'amid': 975,\n",
              " 'inference': 976,\n",
              " 'argue': 977,\n",
              " 'utmost': 978,\n",
              " 'avoid': 979,\n",
              " 'obey': 980,\n",
              " 'sailors': 981,\n",
              " 'exercise': 982,\n",
              " 'suitable': 983,\n",
              " 'taught': 984,\n",
              " 'shepherd': 985,\n",
              " 'scale': 986,\n",
              " 'ambitious': 987,\n",
              " 'task': 988,\n",
              " 'propose': 989,\n",
              " 'practice': 990,\n",
              " 'disposed': 991,\n",
              " 'freemen': 992,\n",
              " 'accomplished': 993,\n",
              " 'deprived': 994,\n",
              " 'assigned': 995,\n",
              " 'watch': 996,\n",
              " 'iron': 997,\n",
              " 'step': 998,\n",
              " 'hour': 999,\n",
              " 'lying': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWagP5ehJCGL"
      },
      "source": [
        "<a id=section603></a>\n",
        "### **6.3 Sequence Inputs and Output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PACdSAPYJAbT",
        "scrolled": true
      },
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences) #array slicing\n",
        "\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "\n",
        "#one hot encode the output word.\n",
        "#Keras provides the to_categorical() that can be used to one hot encode the output words for each input-output sequence pair.\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3E8mJ46Y9ar",
        "outputId": "f9bdd0dc-e039-4249-8e06-6e1a885f0b9f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1046,   11,   11, 1045,  329, 7409,    4,    1, 2873,   35,  213,\n",
              "          1,  261,    3, 2251,    9,   11,  179,  817,  123,   92, 2872,\n",
              "          4,    1, 2249, 7408,    1, 7407, 7406,    2,   75,  120,   11,\n",
              "       1266,    4,  110,    6,   30,  168,   16,   49, 7405,    1, 1609,\n",
              "         13,   57,    8,  549,  151,   11])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwYqY_QdY7-7",
        "outputId": "91f88240-eda7-4508-94e1-220c27f35434"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 7410)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-ePGFcVgEOH",
        "outputId": "89dc7265-f794-4906-bf8a-64dd89bf4044"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 51)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[:,:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi07EJDIlSsP",
        "outputId": "2788d9ba-543f-4fd3-e2cf-bf0d882da1e0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1046,   11,   11, ...,  549,  151,   11],\n",
              "       [  11,   11, 1045, ...,  151,   11,   57],\n",
              "       [  11, 1045,  329, ...,   11,   57, 1147],\n",
              "       ...,\n",
              "       [ 382,  467,    4, ..., 1044,  414,   13],\n",
              "       [ 467,    4,   33, ...,  414,   13,   21],\n",
              "       [   4,   33,   79, ...,   13,   21,   23]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[:,-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhBZZ9yElVDx",
        "outputId": "419108d4-f78d-4ca9-ada3-db9b43244bd5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  57, 1147,   35, ...,   21,   23,   85])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugQoKt9G3FHe",
        "outputId": "324cd50d-ba0e-4f2b-fe4f-3235643e4fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WbqBTEc3FHm",
        "outputId": "59c055bc-a306-4757-a5ba-7edc5ac0ce70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1046,   11,   11, 1045,  329, 7409,    4,    1, 2873,   35,  213,\n",
              "          1,  261,    3, 2251,    9,   11,  179,  817,  123,   92, 2872,\n",
              "          4,    1, 2249, 7408,    1, 7407, 7406,    2,   75,  120,   11,\n",
              "       1266,    4,  110,    6,   30,  168,   16,   49, 7405,    1, 1609,\n",
              "         13,   57,    8,  549,  151,   11])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "easmWXJu3FHv",
        "outputId": "98d64041-cd99-4740-f96c-1b6b2d57b10b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 7410)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G_pEZAf3FHy",
        "outputId": "b7bb852a-52e8-4ce2-875e-5e581c290d64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9PvtEQA3FH0"
      },
      "source": [
        "<a id=section604></a>\n",
        "### **6.4 Fit Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zw5tiv7JKwX"
      },
      "source": [
        "- The learned **embedding** needs to know the size of the **vocabulary** and the length of **input sequences** as previously discussed.\n",
        "\n",
        " - The **output layer** predicts the **next word** as a single **vector** the size of the **vocabulary** with a **probability** for each word in the vocabulary.\n",
        "\n",
        " - A **softmax** activation function is used to **ensure** the outputs have the **characteristics** of normalized probabilities.\n",
        " \n",
        " <center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/images.png\"width=\"400\" height=\"150\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmzS7OUuJIps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f16038-7815-4df6-e52e-1ffa384074bd"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "\"\"\"\" \n",
        "- Size of the **embedding** vector space: a parameter to specify how many dimensions will be used to represent each word\n",
        "\n",
        "- Common values are **50, 100, and 300**. \n",
        "\n",
        "- We will use 50 here, but consider **testing smaller or larger values**.\n",
        "\n",
        "- We will use a two LSTM hidden layers with **100 memory cells** each. \n",
        "\n",
        "- More memory cells and a deeper network may achieve better results.\n",
        "\"\"\"\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "\n",
        "model.add(LSTM(200, return_sequences=True))\n",
        "model.add(LSTM(200))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 50)            370500    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 200)           200800    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 200)               320800    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 200)               40200     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7410)              1489410   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,421,710\n",
            "Trainable params: 2,421,710\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbnFr60_l72M"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-2JH1NlJYcW"
      },
      "source": [
        "**Observation:** \n",
        "\n",
        "- The model is compiled specifying the **categorical** cross **entropy** loss needed to fit the **model**.\n",
        "\n",
        "- Technically, the **model** is learning a **multi-class** classification and this is the **suitable** loss function for this type of problem.\n",
        "\n",
        "- The efficient **Adam** optimizers to **mini-batch** gradient descent is used and **accuracy** is evaluated of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3XPocfhlqpM"
      },
      "source": [
        "- **Model Training** on the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrgQUag_JUwW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05afdbd8-0b08-4bea-a32f-3679560c1ada"
      },
      "source": [
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "927/927 [==============================] - 433s 462ms/step - loss: 6.1028 - accuracy: 0.0776\n",
            "Epoch 2/200\n",
            "927/927 [==============================] - 428s 461ms/step - loss: 5.6115 - accuracy: 0.1134\n",
            "Epoch 3/200\n",
            "927/927 [==============================] - 422s 456ms/step - loss: 5.3729 - accuracy: 0.1348\n",
            "Epoch 4/200\n",
            "927/927 [==============================] - 423s 456ms/step - loss: 5.2225 - accuracy: 0.1491\n",
            "Epoch 5/200\n",
            " 75/927 [=>............................] - ETA: 6:30 - loss: 5.1153 - accuracy: 0.1501"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F85vCF63PGgo"
      },
      "source": [
        "- Use the **Keras model API** to save the model to the file **â€˜model.h5â€˜** in the current working directory.\n",
        "\n",
        "- This is in the **Tokenizer object**, and we can save that too **using Pickle**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TG1hkckPMQg"
      },
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVXdfzw3FIN"
      },
      "source": [
        "----\n",
        "<a id=section7></a>\n",
        "## **7. Use Language model**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq3Xy3B23FIO"
      },
      "source": [
        "\n",
        "<br>   \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow10.png\"width=\"700\" height=\"400\"/></center>\n",
        "\n",
        "<br>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLOxH41PQlx"
      },
      "source": [
        "<a id=section701></a>\n",
        "### **7.1 Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwtQz4N3PZ-1",
        "outputId": "07b33c0f-c86a-4cab-dc52-0f1e0d5b19d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-74c8da55d362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# load cleaned text sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0min_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'republic_sequences.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-74c8da55d362>\u001b[0m in \u001b[0;36mload_doc\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# open the file as read only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# read all text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'republic_sequences.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines[:10]"
      ],
      "metadata": {
        "id": "PmfDw9NMJMrK",
        "outputId": "ab72e4c5-99b3-4b2e-e67a-37ed6621a4f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted',\n",
              " 'i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with',\n",
              " 'went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the',\n",
              " 'down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession',\n",
              " 'yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of',\n",
              " 'to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of the',\n",
              " 'the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of the inhabitants',\n",
              " 'piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of the inhabitants but',\n",
              " 'with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession of the inhabitants but that']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDiucNmFPd02"
      },
      "source": [
        "- We need the text so that we can choose a **source sequence** as input to the model for generating a **new sequence of text**.\n",
        "\n",
        "- The model will require **50 words** as **input**.\n",
        "\n",
        "- Later, we will need to specify the **expected length** of input.\n",
        "\n",
        "- We can determine this from the **input sequences** by **calculating** the length of one line of the loaded data and **subtracting** **1** for the **expected output** word that is also on the same line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_uNuSY7PhVP",
        "outputId": "2e0472c7-cdd0-440d-8cfd-8edaa153293b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "seq_length = len(lines[0].split()) - 1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b3cc82ac3d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'lines' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUEry7f1PjtJ"
      },
      "source": [
        "<a id=section702></a>\n",
        "### **7.2 Load Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3G2SzwJ3FId"
      },
      "source": [
        "- We can now **load the model** from file.\n",
        "\n",
        "\n",
        "- Keras provides the **load_model() function** for loading the model, ready for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAIRVWtaPp2l"
      },
      "source": [
        "\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjdIpsuK3FIg"
      },
      "source": [
        "<a id=section703></a>\n",
        "### **7.3 Generate Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKnsr61-Puwe"
      },
      "source": [
        "* The first step in generating text is **preparing a seed input**.\n",
        "\n",
        "\n",
        "* We will select a **random line** of text from the **input text** for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Kt6rODRcW7DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvyF0mPZHcy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad3eee4-d75f-4ab3-9144-410d219a55a6"
      },
      "source": [
        "\n",
        "\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict probabilities for each word\n",
        "        yhat = np.argmax(model.predict(encoded, verbose=0))\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(\"seed_text:\" + '\\n')\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(\"generated_text:\" + '\\n')\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed_text:\n",
            "\n",
            "that time which the poets call the threshold of old age is life harder towards the end or what report do you give of it i will tell you socrates he said what my own feeling is men of my age flock together we are birds of a feather as the\n",
            "\n",
            "generated_text:\n",
            "\n",
            "good and thirdly aim prevail over them the third trial of conjuring and of the fiction which they is to be unjust and reject the good man will be imagined to be sure he said and in the same time be as well as the good and tyrannical peace at\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3SvYiMo3FIm"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        " - In fact, the **addition** of **concatenation** would help in interpreting the seed and the **generated** text. Nevertheless, the **generated** text gets the right kind of words in the **right** kind of order.\n",
        "\n",
        " - Try running the **example** a few times to see other examples of **generated** text. Let me know in the **comments** below if you see anything interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ViQrJNdLe6_"
      },
      "source": [
        "----\n",
        "<a id=section8></a>\n",
        "## **8. Conclusion**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-oYCORA4oCf",
        "toc-hr-collapsed": false
      },
      "source": [
        "- That **statistical** language models are **central** to many challenging natural language processing tasks.\n",
        "\n",
        "- That state-of-the-art **results** are achieved using **neural language models**, specifically those with **word embeddings** and recurrent neural network algorithms.\n",
        "\n",
        "- In general, **word-level language** models tend to **display** higher accuracy than **character-level language models**. \n",
        "\n",
        "- This is because they can form **shorter** representations of **sentences** and preserve the **context between** words easier than character-level language models.\n",
        "\n",
        "- They allow **conditioning** on increasingly large **context** sizes with only a linear increase in the number of parameters, and they support generalization across **different** contexts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSqqs8h0QWig"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}