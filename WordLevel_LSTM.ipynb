{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lalitpandey02/PythonNotebooks/blob/main/WordLevel_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qTZFFjJkugv"
      },
      "source": [
        "<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBO78ScK3FFx"
      },
      "source": [
        "<center><h1>Introduction to Word Level Language Modelling(Practical Implementation)</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGtLLB_ADRl_"
      },
      "source": [
        "---\n",
        "# **Table of Contents**\n",
        "---\n",
        "\n",
        "**1.** [**Introduction**](#Section1)<br>\n",
        "**2.** [**Problem Description**](#Section2)<br>\n",
        "**3.** [**Installing & Importing Libraries**](#Section3)<br>\n",
        "**4.** [**Data Acquisition & Description**](#Section4)<br>\n",
        "**5.** [**Data Preprocessing**](#Section5)<br>\n",
        "**6.** [**Train Language Model**](#Section6)<br>\n",
        "  - **6.1** [**Load Sequence**](#Section61)\n",
        "  - **6.2** [**Encode Sequence**](#Section62) \n",
        "  - **6.3** [**Sequence Inputs and Output**](#Section63)\n",
        "  - **6.4** [**Fit Model**](#Section61)\n",
        "\n",
        "**7.** [**Use Language Model**](#Section7)<br>\n",
        "  - **7.1** [**Load Sequence**](#Section61)\n",
        "  - **7.2** [**Load Model**](#Section62) \n",
        "  - **7.3** [**Fit Model**](#Section63)\n",
        "\n",
        "**8.** [**Conclusion**](#Section8)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt3g_Q2dZJwN"
      },
      "source": [
        "---\n",
        "<a name = Section1></a>\n",
        "# **1. Introduction**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dVcPtmpNiZJ"
      },
      "source": [
        "- **Language models** learn and **predict** one word at a time. The **training** of the network involves **providing** sequences of words as **input** that are processed one at a time where a **prediction** can be made and learned for each **input sequence**.\n",
        "\n",
        "- Neural Language Models (NLM) address the **N-gram data sparsity** issue through **parameterization** of words as **vectors** (word embeddings) and using them as inputs to a neural network.\n",
        "\n",
        "- Word **embeddings** obtained through NLMs **exhibit** the **property** whereby semantically close **words** are likewise **close** in the induced **vector space**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIpcO8AsTU1i"
      },
      "source": [
        "---\n",
        "<a name = Section2></a>\n",
        "# **2. Problem Statement**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B523lFJxHXT6"
      },
      "source": [
        "- The **problem statement** is to train a **language model** on the given text and then **generate** text given an input text in such a way that it looks **straight** out of this document and is **grammatically** correct and **legible** to read.\n",
        "\n",
        "* For this, we need to develop **word-level** neural language **model** and use  it to generate text.\n",
        "\n",
        "* A **language model** can predict the probability of the next word in the sequence, based on the **words** already **observed** in the sequence.\n",
        "\n",
        "* **Neural network models** are a preferred method for **developing statistical language models** because they can use a **distributed representation** where different words with similar meanings have **similar representation**.\n",
        "\n",
        "- Also, it is because they can use a **large context** of recently observed words when **making predictions**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOmVgY8yWE36"
      },
      "source": [
        "---\n",
        "<a name = Section3></a>\n",
        "# **3. Installing and Importing Libraries**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItkIWd-QuyJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54255f63-5874-49ab-92ab-9a953b3366da"
      },
      "source": [
        "# Import tensorflow 2.x\n",
        "# This code block will only work in Google Colab.\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYgJy8N1Gpd2"
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9OWuHO0WF1n"
      },
      "source": [
        "---\n",
        "<a name = Section4></a>\n",
        "# **4. Data Acquisition & Description**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iIrOcT-Fn6N"
      },
      "source": [
        "- **The Republic by Plato**\n",
        "<br>\n",
        "<center> <img src=\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/socrates.JPG\" /></center>\n",
        "<br>\n",
        "\n",
        "-  The Republic is the **classical Greek philosopher Platoâ€™s** most famous work.\n",
        "\n",
        "- It is **structured** as a **dialog** (e.g. conversation) on the topic of **order and justice** within a city state\n",
        "\n",
        "- Download the ASCII **text version** of the entire book (or books) here: [The Republic](https://https://www.gutenberg.org/ebooks/1497) and save it as *republic.txt*\n",
        "\n",
        "- Open the file in a **text editor** and delete the **front** and **back** matter. \n",
        "\n",
        "- This includes details about the **book** at the beginning, a **long analysis**, and **license** information at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq0Ve7ItvsQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6783db54-c0de-43ea-f0e7-9e50f9aacaff"
      },
      "source": [
        "import urllib\n",
        "response = urllib.request.urlopen('https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/republic_clean.txt')\n",
        "doc = response.read().decode('utf8')\n",
        "print(doc[:800])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ï»¿\rBOOK I.\r\r\n",
            "\r\r\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\r\r\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\r\r\n",
            "Artemis.); and also because I wanted to see in what manner they would\r\r\n",
            "celebrate the festival, which was a new thing. I was delighted with the\r\r\n",
            "procession of the inhabitants; but that of the Thracians was equally,\r\r\n",
            "if not more, beautiful. When we had finished our prayers and viewed the\r\r\n",
            "spectacle, we turned in the direction of the city; and at that instant\r\r\n",
            "Polemarchus the son of Cephalus chanced to catch sight of us from a\r\r\n",
            "distance as we were starting on our way home, and told his servant to\r\r\n",
            "run and bid us wait for him. The servant took hold of me by the cloak\r\r\n",
            "behind, and said: Polemarchus desires you to wait.\r\r\n",
            "\r\r\n",
            "I turn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lszB-fmkWW4f"
      },
      "source": [
        "---\n",
        "<a name = Section5></a>\n",
        "# **5. Data Preprocessing**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvysmaT93FF6"
      },
      "source": [
        "We'll be using the following **process sequence** in this notebook:\n",
        "\n",
        "<br>   \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow0.png\"width=\"600\" height=\"400\"/></center>\n",
        "\n",
        "<br>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKSbwDDO3FGU"
      },
      "source": [
        "\n",
        "#### Clean Text\n",
        "\n",
        "* **Replace â€˜â€“â€˜** with a white space so we can split words better.\n",
        "\n",
        "* **Split words** based on **white space**.\n",
        "\n",
        "* Remove all **punctuation** from **words** to reduce the vocabulary size (e.g. â€˜What?â€™ becomes â€˜Whatâ€™).\n",
        "\n",
        "* **Remove all words** that are not alphabetic to remove standalone **punctuation tokens**.\n",
        "\n",
        "* Normalize **all words** to **lowercase** to reduce the **vocabulary size**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCORHQHPGmTC"
      },
      "source": [
        "\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPb4XvVCGslQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4972be8e-5091-43b8-e45e-08710fa79f39"
      },
      "source": [
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOI6sfwaG8QB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170554f5-9bc3-47bd-e281-225e6d4f50e7"
      },
      "source": [
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "    # select sequence of tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # convert into a line\n",
        "    line = ' '.join(seq)\n",
        "    # store\n",
        "    sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 118633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5YCAlga3FGw",
        "outputId": "5657a86a-8f37-4f17-d55b-d52d4bc3899c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sequences[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf7dLooF3FGp"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- Transforming the tokens into **space-separated strings** for later storage in a file.\n",
        "\n",
        "- Splitting the list of **clean tokens** into **sequences**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RksKge0_G_Oj"
      },
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "    \n",
        "# save sequences to file\n",
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IwlwV4_H3OW"
      },
      "source": [
        "----\n",
        "<a id=section6></a>\n",
        "## **6. Train Language Model**\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDXpKVrv3FG6"
      },
      "source": [
        "\n",
        "<br>   \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow4.png\"width=\"700\" height=\"400\"/></center>\n",
        "\n",
        "<br>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHCFMESq3FG7"
      },
      "source": [
        "* Model uses a **distributed** representation for words so that different words with similar meanings will have a similar representation.\n",
        "\n",
        "* It **learns** the **representation** at the same time as **learning the model.**\n",
        "\n",
        "* It **learns** to predict the **probability** for the next **word** using the **context** of the last **100 words**.\n",
        "\n",
        "- We will use an **Embedding Layer** to learn the representation of words, and a **Long Short-Term Memory (LSTM)** recurrent neural network to learn to **predict words** based on their context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NmWD7bb3FG_"
      },
      "source": [
        "<a id=section601></a>\n",
        "### **6.1 Load Sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBgoos1p3FHB"
      },
      "source": [
        "- We can load our **training data** using the **`load_doc()`** function defined below.\n",
        "\n",
        "\n",
        "- Once loaded, we can **split the data into separate training sequences** by splitting based on new lines.\n",
        "\n",
        "\n",
        "- The snippet below will load the **â€˜republic_sequences.txtâ€˜** data file from the current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYZIaBv-IRDJ"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7kz_wjL3FHH",
        "outputId": "0cc8d2c8-b1df-4115-c4b3-76fd56a4d838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lines[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhFPp4lxIW1E"
      },
      "source": [
        "<a id=section602></a>\n",
        "### **6.2 Encode Sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdcRYdZI3FHN"
      },
      "source": [
        "- The **word embedding layer** expects input sequences to be comprised of integers.\n",
        "\n",
        "- We can **map each word in our vocabulary** to a unique integer and encode our input sequences.\n",
        "\n",
        "- Later, when we make predictions, we can convert the **prediction to numbers** and look up their **associated words** in the **same mapping**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDn6Si02IeI6"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "First, the Tokenizer must be trained on the entire training dataset, which means it finds all of the unique words in the data and assigns each a unique integer.\n",
        "\n",
        "We can then use the fit Tokenizer to encode all of the training sequences, converting each sequence from a list of words to a list of integers.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxR_DnR9I7Zt"
      },
      "source": [
        "- We can access the **mapping** of **words** to **integers** as a dictionary attribute called **`word_index`** on the **tokenizer** object.\n",
        "\n",
        "- We need to know the **size** of the **vocabulary** for defining the **embedding** layer later. \n",
        "\n",
        "- We can determine the vocabulary by **calculating** the size of the **mapping dictionary**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2ZZJ3DxI_2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b39219-21c1-4b14-a085-c19eed417ea4"
      },
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7410"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWagP5ehJCGL"
      },
      "source": [
        "<a id=section603></a>\n",
        "### **6.3 Sequence Inputs and Output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PACdSAPYJAbT",
        "scrolled": true
      },
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences) #array slicing\n",
        "\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "\n",
        "#one hot encode the output word.\n",
        "#Keras provides the to_categorical() that can be used to one hot encode the output words for each input-output sequence pair.\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-ePGFcVgEOH",
        "outputId": "0f0cd0f3-be3e-4529-d674-5fe5550b2956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 51)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[:,:-1]"
      ],
      "metadata": {
        "id": "oi07EJDIlSsP",
        "outputId": "780baf91-0d2b-4344-a043-584503f25404",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1046,   11,   11, ...,  549,  151,   11],\n",
              "       [  11,   11, 1045, ...,  151,   11,   57],\n",
              "       [  11, 1045,  329, ...,   11,   57, 1147],\n",
              "       ...,\n",
              "       [ 382,  467,    4, ..., 1044,  414,   13],\n",
              "       [ 467,    4,   33, ...,  414,   13,   21],\n",
              "       [   4,   33,   79, ...,   13,   21,   23]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[:,-1]"
      ],
      "metadata": {
        "id": "PhBZZ9yElVDx",
        "outputId": "9e2db9d8-b935-4549-efee-9c93b67547a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  57, 1147,   35, ...,   21,   23,   85])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugQoKt9G3FHe",
        "outputId": "a745d588-be71-4af8-fc99-70e3e783d16e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WbqBTEc3FHm",
        "outputId": "1ca306d2-91ce-461c-9fcd-e30b86148e31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1046,   11,   11, 1045,  329, 7409,    4,    1, 2873,   35,  213,\n",
              "          1,  261,    3, 2251,    9,   11,  179,  817,  123,   92, 2872,\n",
              "          4,    1, 2249, 7408,    1, 7407, 7406,    2,   75,  120,   11,\n",
              "       1266,    4,  110,    6,   30,  168,   16,   49, 7405,    1, 1609,\n",
              "         13,   57,    8,  549,  151,   11])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "easmWXJu3FHv",
        "outputId": "63037980-d6fd-409b-9f8d-cdf9f4668372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 7410)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G_pEZAf3FHy",
        "outputId": "5abbbfb3-dd03-47c5-eace-cbf248b2a90c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9PvtEQA3FH0"
      },
      "source": [
        "<a id=section604></a>\n",
        "### **6.4 Fit Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zw5tiv7JKwX"
      },
      "source": [
        "- The learned **embedding** needs to know the size of the **vocabulary** and the length of **input sequences** as previously discussed.\n",
        "\n",
        " - The **output layer** predicts the **next word** as a single **vector** the size of the **vocabulary** with a **probability** for each word in the vocabulary.\n",
        "\n",
        " - A **softmax** activation function is used to **ensure** the outputs have the **characteristics** of normalized probabilities.\n",
        " \n",
        " <center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/images.png\"width=\"400\" height=\"150\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmzS7OUuJIps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f1bca3-2c8d-4a59-ae8f-772ad5f91cba"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "\"\"\"\" \n",
        "- Size of the **embedding** vector space: a parameter to specify how many dimensions will be used to represent each word\n",
        "\n",
        "- Common values are **50, 100, and 300**. \n",
        "\n",
        "- We will use 50 here, but consider **testing smaller or larger values**.\n",
        "\n",
        "- We will use a two LSTM hidden layers with **100 memory cells** each. \n",
        "\n",
        "- More memory cells and a deeper network may achieve better results.\n",
        "\"\"\"\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 50)            370500    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 100)           60400     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7410)              748410    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,269,810\n",
            "Trainable params: 1,269,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbnFr60_l72M"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-2JH1NlJYcW"
      },
      "source": [
        "**Observation:** \n",
        "\n",
        "- The model is compiled specifying the **categorical** cross **entropy** loss needed to fit the **model**.\n",
        "\n",
        "- Technically, the **model** is learning a **multi-class** classification and this is the **suitable** loss function for this type of problem.\n",
        "\n",
        "- The efficient **Adam** optimizers to **mini-batch** gradient descent is used and **accuracy** is evaluated of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3XPocfhlqpM"
      },
      "source": [
        "- **Model Training** on the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrgQUag_JUwW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf650086-9968-45e9-ffd0-7ccf6cf5f3f7"
      },
      "source": [
        "\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "927/927 [==============================] - 54s 47ms/step - loss: 6.1478 - accuracy: 0.0728\n",
            "Epoch 2/100\n",
            "927/927 [==============================] - 15s 16ms/step - loss: 5.6820 - accuracy: 0.1075\n",
            "Epoch 3/100\n",
            "927/927 [==============================] - 14s 15ms/step - loss: 5.4579 - accuracy: 0.1303\n",
            "Epoch 4/100\n",
            "927/927 [==============================] - 13s 14ms/step - loss: 5.3026 - accuracy: 0.1440\n",
            "Epoch 5/100\n",
            "927/927 [==============================] - 13s 14ms/step - loss: 5.1821 - accuracy: 0.1533\n",
            "Epoch 6/100\n",
            "927/927 [==============================] - 13s 14ms/step - loss: 5.0795 - accuracy: 0.1591\n",
            "Epoch 7/100\n",
            "927/927 [==============================] - 13s 14ms/step - loss: 4.9939 - accuracy: 0.1643\n",
            "Epoch 8/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.9144 - accuracy: 0.1686\n",
            "Epoch 9/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.8379 - accuracy: 0.1733\n",
            "Epoch 10/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.7666 - accuracy: 0.1773\n",
            "Epoch 11/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.6975 - accuracy: 0.1805\n",
            "Epoch 12/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.6329 - accuracy: 0.1838\n",
            "Epoch 13/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.5715 - accuracy: 0.1868\n",
            "Epoch 14/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.5127 - accuracy: 0.1886\n",
            "Epoch 15/100\n",
            "927/927 [==============================] - 13s 14ms/step - loss: 4.4579 - accuracy: 0.1918\n",
            "Epoch 16/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.4049 - accuracy: 0.1940\n",
            "Epoch 17/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.3537 - accuracy: 0.1968\n",
            "Epoch 18/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.3066 - accuracy: 0.1990\n",
            "Epoch 19/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.2614 - accuracy: 0.2021\n",
            "Epoch 20/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.2187 - accuracy: 0.2047\n",
            "Epoch 21/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.1778 - accuracy: 0.2072\n",
            "Epoch 22/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.1411 - accuracy: 0.2106\n",
            "Epoch 23/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.1047 - accuracy: 0.2134\n",
            "Epoch 24/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.0721 - accuracy: 0.2155\n",
            "Epoch 25/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 4.0385 - accuracy: 0.2188\n",
            "Epoch 26/100\n",
            "927/927 [==============================] - 13s 14ms/step - loss: 4.0094 - accuracy: 0.2204\n",
            "Epoch 27/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.9792 - accuracy: 0.2238\n",
            "Epoch 28/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.9501 - accuracy: 0.2258\n",
            "Epoch 29/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.9241 - accuracy: 0.2286\n",
            "Epoch 30/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.8978 - accuracy: 0.2311\n",
            "Epoch 31/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.8725 - accuracy: 0.2330\n",
            "Epoch 32/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.8474 - accuracy: 0.2364\n",
            "Epoch 33/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.8243 - accuracy: 0.2376\n",
            "Epoch 34/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.8010 - accuracy: 0.2401\n",
            "Epoch 35/100\n",
            "927/927 [==============================] - 13s 14ms/step - loss: 3.7788 - accuracy: 0.2433\n",
            "Epoch 36/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.7565 - accuracy: 0.2456\n",
            "Epoch 37/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.7343 - accuracy: 0.2480\n",
            "Epoch 38/100\n",
            "927/927 [==============================] - 13s 14ms/step - loss: 3.7131 - accuracy: 0.2508\n",
            "Epoch 39/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.6935 - accuracy: 0.2526\n",
            "Epoch 40/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.6721 - accuracy: 0.2544\n",
            "Epoch 41/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.6538 - accuracy: 0.2571\n",
            "Epoch 42/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.6377 - accuracy: 0.2594\n",
            "Epoch 43/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 3.6173 - accuracy: 0.2623\n",
            "Epoch 44/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.5981 - accuracy: 0.2637\n",
            "Epoch 45/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.5782 - accuracy: 0.2666\n",
            "Epoch 46/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.5597 - accuracy: 0.2689\n",
            "Epoch 47/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.5440 - accuracy: 0.2714\n",
            "Epoch 48/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.5246 - accuracy: 0.2728\n",
            "Epoch 49/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.5042 - accuracy: 0.2753\n",
            "Epoch 50/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.4859 - accuracy: 0.2788\n",
            "Epoch 51/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 3.4685 - accuracy: 0.2801\n",
            "Epoch 52/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.4497 - accuracy: 0.2828\n",
            "Epoch 53/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.4331 - accuracy: 0.2849\n",
            "Epoch 54/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.4156 - accuracy: 0.2880\n",
            "Epoch 55/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.3985 - accuracy: 0.2906\n",
            "Epoch 56/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.3830 - accuracy: 0.2934\n",
            "Epoch 57/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.3648 - accuracy: 0.2949\n",
            "Epoch 58/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.3475 - accuracy: 0.2972\n",
            "Epoch 59/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.3333 - accuracy: 0.2989\n",
            "Epoch 60/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 3.3155 - accuracy: 0.3010\n",
            "Epoch 61/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 3.2994 - accuracy: 0.3031\n",
            "Epoch 62/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.2832 - accuracy: 0.3066\n",
            "Epoch 63/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.2687 - accuracy: 0.3079\n",
            "Epoch 64/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.2526 - accuracy: 0.3102\n",
            "Epoch 65/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.2390 - accuracy: 0.3111\n",
            "Epoch 66/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.2227 - accuracy: 0.3161\n",
            "Epoch 67/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 3.2057 - accuracy: 0.3164\n",
            "Epoch 68/100\n",
            "927/927 [==============================] - 11s 12ms/step - loss: 3.1926 - accuracy: 0.3201\n",
            "Epoch 69/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 3.1774 - accuracy: 0.3213\n",
            "Epoch 70/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.1637 - accuracy: 0.3234\n",
            "Epoch 71/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.1508 - accuracy: 0.3259\n",
            "Epoch 72/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.1337 - accuracy: 0.3290\n",
            "Epoch 73/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.1208 - accuracy: 0.3303\n",
            "Epoch 74/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.1072 - accuracy: 0.3326\n",
            "Epoch 75/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.0902 - accuracy: 0.3350\n",
            "Epoch 76/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 3.0792 - accuracy: 0.3365\n",
            "Epoch 77/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.0635 - accuracy: 0.3401\n",
            "Epoch 78/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 3.0536 - accuracy: 0.3405\n",
            "Epoch 79/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.0404 - accuracy: 0.3433\n",
            "Epoch 80/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 3.0230 - accuracy: 0.3465\n",
            "Epoch 81/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 3.0128 - accuracy: 0.3483\n",
            "Epoch 82/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 2.9993 - accuracy: 0.3493\n",
            "Epoch 83/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.9855 - accuracy: 0.3525\n",
            "Epoch 84/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.9716 - accuracy: 0.3546\n",
            "Epoch 85/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.9619 - accuracy: 0.3560\n",
            "Epoch 86/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.9479 - accuracy: 0.3578\n",
            "Epoch 87/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.9342 - accuracy: 0.3597\n",
            "Epoch 88/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.9227 - accuracy: 0.3614\n",
            "Epoch 89/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 2.9103 - accuracy: 0.3639\n",
            "Epoch 90/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.8991 - accuracy: 0.3662\n",
            "Epoch 91/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 2.8880 - accuracy: 0.3682\n",
            "Epoch 92/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.8726 - accuracy: 0.3706\n",
            "Epoch 93/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.8630 - accuracy: 0.3720\n",
            "Epoch 94/100\n",
            "927/927 [==============================] - 12s 12ms/step - loss: 2.8510 - accuracy: 0.3740\n",
            "Epoch 95/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.8387 - accuracy: 0.3762\n",
            "Epoch 96/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.8264 - accuracy: 0.3781\n",
            "Epoch 97/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.8117 - accuracy: 0.3816\n",
            "Epoch 98/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.8002 - accuracy: 0.3825\n",
            "Epoch 99/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.7904 - accuracy: 0.3848\n",
            "Epoch 100/100\n",
            "927/927 [==============================] - 12s 13ms/step - loss: 2.7770 - accuracy: 0.3861\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4bf758df10>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F85vCF63PGgo"
      },
      "source": [
        "- Use the **Keras model API** to save the model to the file **â€˜model.h5â€˜** in the current working directory.\n",
        "\n",
        "- This is in the **Tokenizer object**, and we can save that too **using Pickle**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TG1hkckPMQg"
      },
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVXdfzw3FIN"
      },
      "source": [
        "----\n",
        "<a id=section7></a>\n",
        "## **7. Use Language model**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq3Xy3B23FIO"
      },
      "source": [
        "\n",
        "<br>   \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow10.png\"width=\"700\" height=\"400\"/></center>\n",
        "\n",
        "<br>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLOxH41PQlx"
      },
      "source": [
        "<a id=section701></a>\n",
        "### **7.1 Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwtQz4N3PZ-1"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDiucNmFPd02"
      },
      "source": [
        "- We need the text so that we can choose a **source sequence** as input to the model for generating a **new sequence of text**.\n",
        "\n",
        "- The model will require **50 words** as **input**.\n",
        "\n",
        "- Later, we will need to specify the **expected length** of input.\n",
        "\n",
        "- We can determine this from the **input sequences** by **calculating** the length of one line of the loaded data and **subtracting** **1** for the **expected output** word that is also on the same line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_uNuSY7PhVP"
      },
      "source": [
        "seq_length = len(lines[0].split()) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUEry7f1PjtJ"
      },
      "source": [
        "<a id=section702></a>\n",
        "### **7.2 Load Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3G2SzwJ3FId"
      },
      "source": [
        "- We can now **load the model** from file.\n",
        "\n",
        "\n",
        "- Keras provides the **load_model() function** for loading the model, ready for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAIRVWtaPp2l"
      },
      "source": [
        "\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjdIpsuK3FIg"
      },
      "source": [
        "<a id=section703></a>\n",
        "### **7.3 Generate Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKnsr61-Puwe"
      },
      "source": [
        "* The first step in generating text is **preparing a seed input**.\n",
        "\n",
        "\n",
        "* We will select a **random line** of text from the **input text** for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvyF0mPZHcy2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "20fd87c6-c646-49bd-8049-b452ea98bcc7"
      },
      "source": [
        "\n",
        "\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict probabilities for each word\n",
        "        yhat = model.predict(encoded, verbose=0)\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(\"seed_text:\" + '\\n')\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(\"generated_text:\" + '\\n')\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed_text:\n",
            "\n",
            "in homer friend sit still and obey my word and the verses which follow the greeks marched breathing prowess in silent awe of their leaders and other sentiments of the same kind we shall what of this line o heavy with wine who hast the eyes of a dog and the\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-be7962b5c28f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# generate new text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generated_text:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-be7962b5c28f>\u001b[0m in \u001b[0;36mgenerate_seq\u001b[0;34m(model, tokenizer, seq_length, seed_text, n_words)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mout_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3SvYiMo3FIm"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        " - In fact, the **addition** of **concatenation** would help in interpreting the seed and the **generated** text. Nevertheless, the **generated** text gets the right kind of words in the **right** kind of order.\n",
        "\n",
        " - Try running the **example** a few times to see other examples of **generated** text. Let me know in the **comments** below if you see anything interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ViQrJNdLe6_"
      },
      "source": [
        "----\n",
        "<a id=section8></a>\n",
        "## **8. Conclusion**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-oYCORA4oCf",
        "toc-hr-collapsed": false
      },
      "source": [
        "- That **statistical** language models are **central** to many challenging natural language processing tasks.\n",
        "\n",
        "- That state-of-the-art **results** are achieved using **neural language models**, specifically those with **word embeddings** and recurrent neural network algorithms.\n",
        "\n",
        "- In general, **word-level language** models tend to **display** higher accuracy than **character-level language models**. \n",
        "\n",
        "- This is because they can form **shorter** representations of **sentences** and preserve the **context between** words easier than character-level language models.\n",
        "\n",
        "- They allow **conditioning** on increasingly large **context** sizes with only a linear increase in the number of parameters, and they support generalization across **different** contexts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSqqs8h0QWig"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}